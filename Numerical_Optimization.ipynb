{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa0a1fb",
   "metadata": {},
   "source": [
    " # Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c3fd2",
   "metadata": {},
   "source": [
    "Consider the following minimization problem:\n",
    "    $$\n",
    "    \\min_{x\\in\\mathbb{R}^d} f(x),\n",
    "    $$\n",
    "where $f:\\mathbb{R}^d\\to\\mathbb{R}$ is a smooth convex function.\n",
    "\n",
    "The simplest approach to solve this problem is the gradient descent where we generate a sequence $(x_k)_k$ as follows:\n",
    "$$\n",
    "x_{k+1} = x_{k} - \\gamma\\nabla f(x_{k}),\n",
    "$$\n",
    "where $\\nabla f$ is the gradient of $f$, $x_{0}$ is an arbitrary point to initialize the iterations and $\\gamma$ is the step size. To have convergence the step size must satisfy $0<\\gamma<2/L$, where $L$ is the smoothness parameter of $f$, i.e., the Lipschitz constant of $\\nabla f$. In particular, if $f\\in C^{2}$, we have that $\\gamma = \\sup_{x}\\Vert Hf(x)\\Vert$ where $Hf(x)\\in\\mathcal{M}_{d,d}$ is the Hessian matrix of $f$ at $x$ and $\\Vert\\cdot\\Vert$ is the spectral operator norm.\n",
    "\n",
    "## Exercise 1: \n",
    "Consider the following function $f(x,y) = \\frac{1}{2}(\\alpha x^2 + \\beta y^2)$ where $\\alpha,\\beta>0$ are parameter controlling the anisotropy of the problem. In numerical optimization, we usually define *oracles* for the functions. These are functions that take a point of the ambiant space as an input and return the value of the function at this point (or its gradient, Hessian, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade831b",
   "metadata": {},
   "source": [
    "> **Question a:** Define oracles of f and its gradient with $\\alpha = 1$ and $\\beta = 8$. Plot the level sets of $f$.\n",
    "\n",
    "> **Question b:** Implement a constant stepsize gradient method `gradient_descent(f,grad_f,x0,gamma,,itermax)` that takes:\n",
    "> * `f` and `grad_f`: respectively functions and gradient oracles\n",
    "> * `x0`: a starting point\n",
    "> * `gamma`: stepsize\n",
    "> * ``itermax`: maximum number of iterations;\n",
    ">\n",
    "> and returns a tuple with `x` the final point.\n",
    "Implement a function `gradient_descent_array` that puts all vectors of iterates in a matrix.\n",
    "\n",
    "> **Question c:** Plot the cost function $f(x_k)$ as function of $k$ and plot the iterates above the contourplot of $f$.\n",
    "> **Question d:** Perform the same tasks with i) different values of $\\gamma$, ii) different initial points $x_0$ and iii) different parameters $\\alpha, \\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cb00e",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "We consider two $\\mathbb{R}^2 \\to \\mathbb{R}$ function\n",
    "$$\n",
    "f(x,y) = 4 (x-3)^2 + 2(y-1)^2~~\\mbox{and}~~g(x,y) = \\log( 1 + \\exp(4 (x-3)^2 ) + \\exp( 2(y-1)^2 ) ) - \\log(3)\n",
    "$$\n",
    "with are convex and have same global minimizer $(3,1)$. First plot the $3D$ shapes of $f$ and $g$ as well as their contours. \n",
    "\n",
    "> **Question a:** As in `Exercise 1`, define oracles for $f$, $g$ and their gradients.\n",
    "\n",
    "> **Question b:** Test your gradient descent function on $f$ and $g$:\n",
    "> 1. Verify that the final point is close to the minimizer $(3,1)$ \n",
    "> 2. Plot the iterates abover the contourplots of $f$ and $g$\n",
    "> 3. Change the stepsize and give the values for which the algorithm i) diverges; and ii) oscillates. Compare with theoretical limits by computing the Lipschitz constant of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d257a",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "In this example, we use linear algebra to extract information from data; more precisely, we predict final notes of a group of student from their profiles with the [Student Performance dataset](https://archive.ics.uci.edu/ml/datasets/Student+Performance) which includes secondary education students of two Portuguese schools.\n",
    "\n",
    "\n",
    "Profiles include features such as student grades, demographic, social and school related features and were collected by using school reports and questionnaires. There are $m = 395$ students (examples) and we selected $n = 27$ features (see `data/student.txt` for the features description and `datat/student-mat.csv` for the csv dataset.)\n",
    "\n",
    "\n",
    "Our goal is to predict a target feature (the $28$-th) which is the final grade of the student from the other features (the first $27$). We assume that the final grade can be explained by a linear combination of the other features. We are going to learn from this data using linear regression over the $m_{learn} = 300$ students (called the *learning set*). We will check our prediction by comparing the results for the other $m_{test} = 95$ students (the *testing set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4933b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# File reading\n",
    "dat_file = np.load('data/student.npz')\n",
    "A_learn = dat_file['A_learn']\n",
    "b_learn = dat_file['b_learn']\n",
    "A_test = dat_file['A_test']\n",
    "b_test = dat_file['b_test']\n",
    "\n",
    "m = 395 # number of read examples (total:395)\n",
    "n = 27 # features\n",
    "m_learn = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a62f96",
   "metadata": {},
   "source": [
    "Mathematically, from the $m_{learn} \\times (n+1)$ *learning matrix* (the number of columns is $n+1$ as a column of ones, called *intercept* for statistical reasons). $A_{learn}$ comprising of the features values of each training student in line, and the vector of the values of the target features $b_{learn}$;  we seek a size-$n+1$ *regression vector* that minimizes the squared error between  $A_{learn} x$ and $b_{learn}$. This problem can we written as a least square problem:\n",
    "$$ \\min_{x\\in\\mathbb{R}^{n+1}}  s(x):=\\|  A_{learn} x - b_{learn} \\|_2^2 . $$\n",
    "\n",
    "> **Question a:** Compute numerically the rank of the $m_{learn} \\times (n+1)$ matrix $A_{learn}$. Conclude about the existence and uniqueness of solutions of the problem.\n",
    "\n",
    "> **Question b:** Compute the solution of the minimization problem using the least square solver of Numpy [lstsq].\n",
    "\n",
    "> **Question c:** Construct the oracles for function $s$ and its gradient as in `Exercise 1`.\n",
    "\n",
    "> **Question d:** Find a solution to the minimization of $s$ using your gradient algorithm. Compare with Numpy's Least Square routine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
